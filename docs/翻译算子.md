# 翻译框架



## huggingFace

一个 **完整的神经机器翻译（NMT）框架**（训练+推理），支持加载、训练、微调各种 Transformer 模型（翻译、问答、文本生成等）

比起CTranslat功能复杂，性能和显存都较弱

## opennmt

开源的神经机器翻译（NMT, Neural Machine Translation）工具包，支持训练和部署多语言翻译模型，

###  OpenNMT-py

通用 NMT 框架（训练+推理），基于PyTorch

官网声明，不再维护，转到新项目eole了

https://github.com/OpenNMT/OpenNMT-py?tab=readme-ov-file opennmt-PY

### eole

https://github.com/eole-nlp/eole 

新项目还不够成熟

### **OpenNMT-tf**

通用 NMT 框架（训练+推理），基于TensorFlow

### CTranslate2

 **OpenNMT 团队后来开发的高性能推理引擎**，专门针对 **已训练好的模型的推理部署**

https://opennmt.net/CTranslate2/quickstart.html

https://github.com/OpenNMT/CTranslate2

优点：功能单一，性能好，显存低，大多数模型都能转为给这个库用

缺点：不支持npu，有人开发了这个版本，没有被合入，另起了一个库

## marian-nmt

高效的神经机器翻译系统，高性能、低内存

https://github.com/marian-nmt/marian

2年没更新了

c++的，不支持npu

## sockeye

基于 PyTorch 的神经机器翻译的序列到序列框架

不支持npu，项目部开发功能，进入维护阶段

https://github.com/awslabs/sockeye

## fairseq

性能一般，但项目star很多，为研究专用的，不为生产环境设计

[fairseq](https://github.com/facebookresearch/fairseq)

## easynmt（✅）

只支持几种模型，而且2年没维护了

但是看起来功能不错

- 该软件包为100多种语言提供了易于使用的最先进的机器翻译。该方案的重点是:
- 易于安装和使用:使用最先进的机器翻译，只需3行代码
- 自动下载预训练的机器翻译模型
- 150多种语言的翻译
- 自动语言检测170+语言
- 句子和文件翻译
- 多gpu和多进程翻译

https://github.com/UKPLab/EasyNMT?tab=readme-ov-file#Opus-MT

## 总结

CTranslate2转为生产环境高性能翻译部署设计，功能单一

其他的要么以研究为主，要么功能过多

没有看到有对华为昇腾显卡支持的

# 模型

## 来源

1. opennmt--py model

模型语言支持少，只用于部署验证

https://opennmt.net/Models-py/ 官网

https://forum.opennmt.net/ 论坛

2. huggin face

模型挺全，来源主力

https://huggingface.co/models?sort=trending&search=en-zh

3. opus

全球最大开源平行语料库（几十亿句子对）,提供基于 **MarianNMT** 训练的多语言翻译模型

https://opus.nlpl.eu/Tatoeba/en&cmn/v2023-04-12/Tatoeba

官方训练的 MarianMT 模型全集

https://github.com/Helsinki-NLP/Opus-MT-train

4. NLLB（No Language Left Behind）

https://github.com/facebookresearch/fairseq/tree/nllb

## 模型指标

```php
BLEU+case.mixed+numrefs.1+smooth.exp+tok.zh+version.1.4.2 = 31.4
64.6/41.8/28.2/19.8 (BP = 0.896 ratio = 0.901 hyp_len = 99507 ref_len = 110468)
```

**BLEU (Bilingual Evaluation Understudy)**
 衡量机器翻译结果和人工参考译文的 n-gram 重合度，分数越高，表示翻译和参考越接近。

- **31.4** → 表示整体 BLEU 分数（在 0~100 间），**30+** 算可用但不是顶尖（高质量商用翻译 BLEU 往往在 40 以上）。

**64.6 / 41.8 / 28.2 / 19.8**
 这是 **1-gram / 2-gram / 3-gram / 4-gram** 的匹配率。
 例如 1-gram 64.6% → 单个词的匹配率；4-gram 19.8% → 四个连续词都匹配的比例。

**BP (Brevity Penalty)**
 惩罚翻译太短的情况。BP = 0.896 表示翻译平均比参考稍短（约 10%）。

**ratio = 0.901**
 翻译长度 / 参考译文长度 ≈ 90.1%

**hyp_len / ref_len**

- hyp_len = 模型输出的总词数（这里是分词后的 token 数）
- ref_len = 参考译文的总词数

**预训练模型**

并不是直接拿来翻译的工具，而是需要在特定翻译语料上再“微调（fine-tune）”，才能在翻译任务上发挥最佳效果。

它学到的是“跨语言的语义表示能力”，没有专门针对某个翻译方向（如 En→Zh）做过训练

类似 “mbart-large-50-finetuned-xx-yy” 才是可直接用的

## 中英模型

1. Helsinki-NLP/opus-mt-en-zh  https://huggingface.co/Helsinki-NLP/opus-mt-en-zh

约 298M 参数（中等规模）

领域适配性一般、长句和复杂结构翻译不稳定（超过40个词的长句）、数据老，新词汇不准确

模型倾向于在标点处中断翻译，建议将文本拆分后逐句翻译，再合并结果。

**BLEU**：31.4

**chrF2**：0.268

2. Tatoeba  v2023-04-12

效果类似Helsinki-NLP/opus-mt-en-zh， 不过要数据要新一些

## 多语言模型

### nllb-200(不可商用，支持语言最多)

一个研究模型，没有发布用于生产部署。主要用于机器翻译研究，不打算用于特定领域的文本，不打算用于文档翻译，输入长度不超过512个标记

模型下载 https://huggingface.co/facebook/nllb-200-3.3B 

这个页面i是opennmt官方提供的已被转换为CTranslate2格式的模型/分词器下载，和使用示例，但是这个协议是不可商用

https://forum.opennmt.net/t/nllb-200-with-ctranslate2/5090

### m2m100_1.2B(MIT协议，100多种语言)

支持多语言互相转换

https://huggingface.co/facebook/m2m100_1.2B

### mbart-large-50-many-to-many-mmt（MIT，50种语言）

https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt

### emma-500-llama3.1-8b-bi

https://huggingface.co/MaLA-LM/emma-500-llama3.1-8b-bi

8b fp32，大概要32g显存

支持500多种语言

 不可用ctranslate2部署

### Qwen-MT

支持90多种语言，25.7推出，当前还没开源

## 经英文中转翻译的模型

.。



## 语言识别

facebook/fasttext-language-identification 语言识别器

https://huggingface.co/facebook/fasttext-language-identification



# 参考

https://zhuanlan.zhihu.com/p/690631615